(window["webpackJsonp"]=window["webpackJsonp"]||[]).push([["chunk-2d0b68f8"],{"1e4b":function(e,n,t){"use strict";t.r(n);var i=function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",[t("div",{directives:[{name:"marked",rawName:"v-marked",value:e.md,expression:"md"}],staticClass:"markdown-body"},[e._v("README")])])},o=[],a={name:"help",data:function(){return{md:"# autotimecode\n\nVideo to aligned timecode(SRT), transcription and translation in 4 clicks.\n\nMinimal intrusive to your current workflow. Granular API exposure. Modularized. \n\n## Run it!\n\nMake sure you have Docker Compose installed: refer to https://docs.docker.com/compose/install/ for instruction. Of course you also need Docker - refer to https://docs.docker.com/install/ for instructions.  \n\nConfigure `CELERY_BROKER_URL` and `MONGO_URL` in environment variable, and run\n\n```\ndocker-compose build && docker-compose up\n```\n\nWait till `DeepSegment is loaded!` and `DeepCorrect is loaded!` shows up. This may take longer on CPU machines.\n\nAPI documentations are located in https://github.com/cnbeining/autotimecode/blob/master/autotimecode_api/README.MD .\n\n## Recommended Subtitle Workflow\n\nNote this workflow is based on ACICFG's recommendation: adjust to fit you needs.\n\n1. Get a rough version of timecode from video: covered in this project, `/vad/` endpoint\n2. Transcribe the video (with help of STT). Roughly edit the SRT to include any time range that may be missing from the 1st step: Model building is **NOT** the target of this project - check `/stt/` endpoint for voice recognition helper.\n3. From transcribed SRT, generate SRT with accurate timecode: `/fa/` endpoint.\n4. Continue on translation (maybe with help of Machine Translation): check `/nmt/` endpoint. \n\n## Background\n\nThis project is solving 4 problems:\n\n1. Given video, generate timecode on when human speech exists;\n2. Given video and timecode, transcribe the video automatically;\n3. Given rough timecode, generate accurate timecode aligned with video;\n4. Given transcription, generate translation.\n\n## FAQ\n\n### Where is Speech to Text(STT)?\n\nA STT helper is added at `/stt/` endpoint.\n\nSTT model training is out of the scope of this project, as this project is focusing on timecode generating and aligning. \n\n### Why include Kaldi and ffmpeg twice in different images?\n\n1. The target is that every segment of this project shall be reusable: \n2. Those 2 Kaldies are not in the same version. Same reason I passed [PyKaldi](https://github.com/pykaldi/pykaldi).\n\n### Docker Compose is taking a minute to come up!\n\nTensorFlow Serving does not really mix with custom Keras layers.\n\n### How can I finetune your models?\n\nStay tuned.\n\n### Where is Japanese/Chinese/xxxese/xxxlish support?\n\nThe authors are working hard to make it happen. Again, stay tuned!\n\n## TODO\n\n- [ ] Multiple language support\n- [ ] Add Google Drive support\n- [ ] Add ASS download support\n\n## Authors\n\n- David Zhuang, https://www.cnbeining.com/ , https://github.com/cnbeining . Coded this thing. Productionalized the ML models involved. \n- Yuan-Hang Zhang, https://www.sailorzhang.com/ , https://github.com/sailordiary . Designed the ML algos.\n\nThe authors are member of, and acknowledge the help from [ACICFG](https://www.chineseaci.com).\n\n## License\n\nGPL 3.0. Please contact authors if you need licensing. \n\nPlease retrieve copies of licenses from respected repo links. \n\n`Gentle` is written by @lowerquality, MIT license, https://github.com/lowerquality/gentle .\n\n`Kaldi` is located at https://kaldi-asr.org/ , Apache 2.0 license.\n\n`ffsend.py` was originally written by Robert Xiao (nneonneo@gmail.com), https://github.com/nneonneo/ffsend, and is licensed under the Mozilla Public License 2.0. If you have concern, remove this file and disable Firefox Send.\n\n`ffsend` binary is provided by Tim Vis√©e, https://github.com/timvisee/ffsend , GPL 3.0. If you have concern, please remove this file.\n\nVAD engine is based on work of Hebbar, R., Somandepalli, K., & Narayanan, S. (2019). Robust Speech Activity Detection in Movie Audio: Data Resources and Experimental Evaluation. ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). doi: 10.1109/icassp.2019.8682532 . Original code can be retrieved at https://github.com/usc-sail/mica-speech-activity-detection .\n\n`txt2txt`, `deepcorrect` and `deepsegment` were written by Bedapudi Praneeth, https://github.com/bedapudi6788 , GPL 3.0.\n\nSTT and NMT technologies are provided by Google.\n\nSome STT code are originally from https://github.com/agermanidis/autosub , MIT license.\n"}}},s=a,r=t("6691"),d=Object(r["a"])(s,i,o,!1,null,null,null);n["default"]=d.exports}}]);